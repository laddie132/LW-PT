# config file

dataset:
  h5_path: data/aapd.h5
  data_path: data/aapd.pickle
  meta_data_path: data/aapd.pickle.meta
  doc_rep_path: data/aapd_doc_qt_rep.pt

global:
  random_seed: 123
  num_data_workers: 16

  max_sent_length: 20
  max_sent_num: 40
  max_doc_length: 500
  cand_doc_size: 4

model:
  name: LW_BiGRU_QT      # BiGRU, LW_BiGRU, LW_BiGRU_QT, HAN, HANLG, HLWAN, HLWAN_QT
  hierarchical: False  # same as the model

  use_pretrain: True
  embedding_num: 50000
  embedding_dim: 256
  embedding_path: data/aapd_word2vec.model.wv.vectors.npy
  embedding_freeze: True

  hidden_size: 256
  label_size: 54    # different with dataset
  dropout_p: 0.1
  layer_norm: true

train:
  num_epochs: 10
  train_iters: 20000
  valid_iters: 50
  test_iters: 1000

  eval_steps: 100
  save_steps: 1000

  batch_size: 64
  optimizer: adam   # adam, sgd, adamax
  learning_rate: 0.001
  clip_grad_norm: 5

  learning_rate_decay: 0.5
  start_decay_at: 2
