# config file

dataset:
  h5_path: data/rmsc.h5
#  data_path: data/rmsc_non_hier.pkl
  data_path: data/rmsc.pkl
  meta_data_path: data/rmsc.pkl.meta
  doc_rep_path: data/rmsc_qt_rep.pt-cand3

global:
  random_seed: 123
  num_data_workers: 16

  max_sent_length: 20
  max_sent_num: 40
  max_doc_length: 500
  cand_doc_size: 5

model:
  name: LWQT    # CNN, RNN, LWAN, LWQT
  decoder: MLP  # MLP, LW, LG, LWLG
  hierarchical: True  # same as dataset
  fine_tune: False

  use_pretrain: True
  embedding_num: 50000
  embedding_dim: 100
  embedding_path: data/rmsc_word2vec.model.wv.vectors.npy
  embedding_freeze: True

  cell: GRU
  num_layers: 1
  hidden_size: 100
  label_size: 22    # different with dataset
  dropout_p: 0.2
  layer_norm: true

train:
  num_epochs: 20
  train_iters: 5000
  valid_iters: 50
  test_iters: 1000

  eval_steps: 100
  save_steps: 1000

  batch_size: 128
  optimizer: adam   # adam, sgd, adamax
  learning_rate: 0.001
  clip_grad_norm: 10

  learning_rate_decay: 0.9
  start_decay_at: 2
  decay_steps: 1000